{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets - Hugginface library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\n",
    "print(f\"The first 10 are: {all_datasets[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = emotions[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From datasets to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = emotions[\"train\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2str(row):\n",
    "    return emotions[\"train\"].features['label'].int2str(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label name\"] = df[\"label\"].apply(label2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label name'].value_counts(ascending=True).plot().barh()\n",
    "plt.title(\"Frequency of classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long are the Tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(\"Words Per Tweet\", by=\"label name\", grid=False, showfliers=False, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample text for Transformer neural network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 's', 'a', 'm', 'p', 'l', 'e', ' ', 't', 'e', 'x', 't', ' ', 'f', 'o', 'r', ' ', 'T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a token2idx dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{key: value for (key, value) in iterable} <-> Dict comprehension\n",
    "token2idx = {char: idx for (char, idx) in enumerate(sorted(tokenized_text))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: ' ',\n",
       " 2: ' ',\n",
       " 3: ' ',\n",
       " 4: ' ',\n",
       " 5: ' ',\n",
       " 6: ' ',\n",
       " 7: ' ',\n",
       " 8: 'T',\n",
       " 9: 'T',\n",
       " 10: 'a',\n",
       " 11: 'a',\n",
       " 12: 'a',\n",
       " 13: 'a',\n",
       " 14: 'e',\n",
       " 15: 'e',\n",
       " 16: 'e',\n",
       " 17: 'e',\n",
       " 18: 'e',\n",
       " 19: 'f',\n",
       " 20: 'f',\n",
       " 21: 'h',\n",
       " 22: 'i',\n",
       " 23: 'i',\n",
       " 24: 'k',\n",
       " 25: 'l',\n",
       " 26: 'l',\n",
       " 27: 'm',\n",
       " 28: 'm',\n",
       " 29: 'n',\n",
       " 30: 'n',\n",
       " 31: 'n',\n",
       " 32: 'o',\n",
       " 33: 'o',\n",
       " 34: 'o',\n",
       " 35: 'p',\n",
       " 36: 'r',\n",
       " 37: 'r',\n",
       " 38: 'r',\n",
       " 39: 'r',\n",
       " 40: 'r',\n",
       " 41: 'r',\n",
       " 42: 's',\n",
       " 43: 's',\n",
       " 44: 's',\n",
       " 45: 's',\n",
       " 46: 't',\n",
       " 47: 't',\n",
       " 48: 't',\n",
       " 49: 'u',\n",
       " 50: 'w',\n",
       " 51: 'x'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: ' ',\n",
       " 2: ' ',\n",
       " 3: ' ',\n",
       " 4: ' ',\n",
       " 5: ' ',\n",
       " 6: ' ',\n",
       " 7: ' ',\n",
       " 8: 'T',\n",
       " 9: 'T',\n",
       " 10: 'a',\n",
       " 11: 'a',\n",
       " 12: 'a',\n",
       " 13: 'a',\n",
       " 14: 'e',\n",
       " 15: 'e',\n",
       " 16: 'e',\n",
       " 17: 'e',\n",
       " 18: 'e',\n",
       " 19: 'f',\n",
       " 20: 'f',\n",
       " 21: 'h',\n",
       " 22: 'i',\n",
       " 23: 'i',\n",
       " 24: 'k',\n",
       " 25: 'l',\n",
       " 26: 'l',\n",
       " 27: 'm',\n",
       " 28: 'm',\n",
       " 29: 'n',\n",
       " 30: 'n',\n",
       " 31: 'n',\n",
       " 32: 'o',\n",
       " 33: 'o',\n",
       " 34: 'o',\n",
       " 35: 'p',\n",
       " 36: 'r',\n",
       " 37: 'r',\n",
       " 38: 'r',\n",
       " 39: 'r',\n",
       " 40: 'r',\n",
       " 41: 'r',\n",
       " 42: 's',\n",
       " 43: 's',\n",
       " 44: 's',\n",
       " 45: 's',\n",
       " 46: 't',\n",
       " 47: 't',\n",
       " 48: 't',\n",
       " 49: 'u',\n",
       " 50: 'w',\n",
       " 51: 'x'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token2idx = {char: idx for (idx, char) in enumerate(sorted(set(tokenized_text)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'T': 1,\n",
       " 'a': 2,\n",
       " 'e': 3,\n",
       " 'f': 4,\n",
       " 'h': 5,\n",
       " 'i': 6,\n",
       " 'k': 7,\n",
       " 'l': 8,\n",
       " 'm': 9,\n",
       " 'n': 10,\n",
       " 'o': 11,\n",
       " 'p': 12,\n",
       " 'r': 13,\n",
       " 's': 14,\n",
       " 't': 15,\n",
       " 'u': 16,\n",
       " 'w': 17,\n",
       " 'x': 18}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a list of indexes to the tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [token2idx[char] for char in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 6, 14, 0, 6, 14, 0, 2, 0, 14, 2, 9, 12, 8, 3, 0, 15, 3, 18, 15, 0, 4, 11, 13, 0, 1, 13, 2, 10, 14, 4, 11, 13, 9, 3, 13, 0, 10, 3, 16, 13, 2, 8, 0, 10, 3, 15, 17, 11, 13, 7]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONE HOT Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tensor = torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding = F.one_hot(input_ids_tensor, num_classes=len(token2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 19])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 1\n",
      "One-hot: tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids_tensor[0]}\")\n",
    "print(f\"One-hot: {one_hot_encoding[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample Transformer neural network input text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'Transformer', 'neural', 'network', 'input', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals list:\n",
    "\n",
    "1. token2idx dictionary\n",
    "2. list of indexies of tokens as list\n",
    "3. one-hot encoding of that above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {idx: token for token, idx in enumerate(sorted(set(tokenized_text)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 0, 'Transformer': 1, 'a': 2, 'input': 3, 'is': 4, 'network': 5, 'neural': 6, 'sample': 7, 'text': 8}\n"
     ]
    }
   ],
   "source": [
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_indexies = [token2idx[token] for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 2, 7, 1, 6, 5, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "print(input_indexies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "torch.Size([9, 9])\n"
     ]
    }
   ],
   "source": [
    "tensor_input = torch.tensor(input_indexies)\n",
    "one_hot_word_based = F.one_hot(tensor_input, len(tokenized_text))\n",
    "print(one_hot_word_based)\n",
    "print(one_hot_word_based.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
